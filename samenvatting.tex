\documentclass{2wn20summary}
\usepackage{pgfplots}

\begin{document}
	\maketitle
	\thispagestyle{empty}
	\newpage
	
	\section{Inleiding}
	
	Mocht je foutjes/opmerkingen/verbeteringen vinden, we zien ze graag op de \href{https://github.com/PHPirates/2WN20-summary/issues}{issue tracker}.
	
	\section{College 1}
	\subsection{Floats}
		\indx{Floats} zijn van de vorm $\pm (0.d_1 d_2 d_3 \dots d_n)\beta^e = \pm  (d_1 \beta^{e-1}+d_2 \beta^{e-2}+\dots + d_n \beta^{e-n})$ met $e \in [L,U], L<~0, U>~0$ de exponent, $\beta$ de basis (voor computers $2$), $(0.d_1 d_2 d_3 \dots d_n)$ de \indx{mantisse}. Vaak geldt $\abs{L},\abs{U} \in [100,10000]$. Verder geldt $ 0 < d_1 < \beta $.
		
		\begin{define}
		Voor $n$ zijn in principe twee mogelijkheden, $n=24$ met $0<d_i<\beta, i=2,3,\dotsc ,n$ heet \indx{single precision}, $n=53$ met $0 \le d_i < \beta$ heet \indx{double precision}. 
		\end{define}
		
		\begin{define}
			\[
				\abs{\fl(x)-x} \le \frac{1}{2} \beta^{e-n} \text{ heet een \indx{absolute rounding error},}
			\]
			\[
				\epsilon = \frac{\fl(x) - x}{x} \implies \fl(x) = x(1 + \epsilon) \text{ heet \indx{relative rounding error},}
			\]
			\[ 
				\abs{\epsilon} = \frac{\abs{\fl (x) - x}}{\abs{x}} \leq \beta^{1-n} \text{ heet \indx{machine precision}.}
			\]
		\end{define}
		
	\subsection{Rekenen met floats (ten opzichte van rekenen met de exacte waarde)}
		\begin{define}
			Rekenen met floats gaat als volgt, $ \fl (\fl (x) \circ \fl (y)) $ waar $ \circ \in \{+,-,\times, \div\} $. We bekijken het verschil met de exacte waarde. 
			\begin{align*}
				\abs{ x \circ y - \fl (\fl (x) \circ \fl (y)) } &= \abs{ x \circ y - \fl (x) \circ \fl (y) + \fl (x) \circ \fl (y) - \fl (\fl (x) \circ \fl (y))} \\
				&\leq \abs{x \circ y - \fl (x) \circ \fl (y)} + \abs{\fl (x) \circ \fl (y) - \fl (\fl (x) \circ \fl (y))}.
			\end{align*}
			$ \abs{x \circ y - \fl (x) \circ \fl (y)} $ heet de \indx{transmission error} of \indx{doorwerkingsfout}. Deze heeft te maken met de afronding.
			
			$  \abs{\fl (x) \circ \fl (y) - \fl (\fl (x) \circ \fl (y))} $ heet de \indx{calculation error} of \indx{bewerkingsfout}.
		\end{define}
		
		\begin{note}
			Voor optellen $ (+) $ en aftrekken $ (-) $ is de absolute error relevant. Voor vermenigvuldigen $ (\times) $ en delen $ (\div) $ is de relative error relevant.
		\end{note}
		
	\subsection{\indx{Landau orde symbool}}
		\begin{define}
			$ f(x) = \bigO (g(x)) $ for $ x \to 0 $. Then there exist an $ M > 0, r>0 $ such that
			\[ 
				\forall_{x \in [-r,r]}:\abs{f(x)} \leq M \abs{g(x)}\,.
			\]
		\end{define}
		
		\begin{theorem}
			Stel $ f(x) = \bigO(x^p), g(x) = \bigO(x^q) $ met $ p>0, q>0 $. Dan geldt het volgende:
			\begin{enumerate}[(i)]
				\item $ f(x) = \bigO(x^s) \qquad 0 \leq s < p $
				\item $ \alpha f(x) + \beta g(x) = \bigO (x^{\min (p,q)}) $
				\item $ f(x)g(x) = \bigO(x^{p+q}) $
				\item $ \frac{f(x)}{x^s} = \bigO(x^{p-s}) \qquad 0 \leq s < p $.
			\end{enumerate}
		\end{theorem}
		
	\subsection{Taylorpolynoom}
		\begin{theorem}
			Stel $ f(x) \in C^{(n+1)}[a,b] $ met $ x \in (a,b) $ en $ c \in (a,b) $.	Dan is er een $ \xi $ tussen $x$ en $c$ zodanig dat
			\begin{align*}
				f(x) &= P_n(x) + R_n(x) \\ 
				&= f(c) + (x-c)f'(c) + \frac{(x-c)^2}{2!}f''(c) + \dots + \frac{(x-c)^n}{n!}f^n(c) + \frac{(x-c)^{n+1}}{(n+1)!}f^{(n+1)}(\xi).
			\end{align*}
			We noemen dan $ \frac{(x-c)^{n+1}}{(n+1)!}f^{(n+1)}(\xi) $ de \indx{foutterm}\index{error Taylor polynomial}.
		\end{theorem}

	\newpage
	\section{College 2}
	\subsection{Interpolatie}
		\begin{define}
			Stel $ f(x) \in C[a,b] $ en $ x_0, x_1 \in (a,b) $, dan geldt voor het \indx{interpolatiepolynoom} $ p(x) $ dat
			\begin{align*}
				p(x) &= f(x_0) + \frac{f(x_1) - f(x_0)}{x_1 - x_0} \\
					&= \frac{x-x_1}{x_0-x_1}f(x_0) + \frac{x-x_0}{x_1-x_0}f(x_1)\,.
			\end{align*}
			
			\begin{tikzpicture}
			\begin{axis}[
				ylabel=$f(x)$,
				ytick={2,4},
				yticklabels={$f(x_0)$,$f(x_1)$},
				xlabel={$x$},
				xtick={0,2,4,8},
				xticklabels={$a$,$x_0$,$x_1$,$b$}
			]
				\addplot[samples=200, red, domain=0:8]{3 + sin(deg(2*x)} node[below] {$f(x)$};
				% p(x):
				\addplot[samples=20, dashed, domain=0:2]{x};
				\addplot[samples=20, domain=2:4]{x} node[above,pos=1]{$p(x)$};
				\addplot[samples=20, dashed, domain=4:8]{x};
			\end{axis}
			\end{tikzpicture}
		\end{define}
		
		\paragraph{Analyse fout in $ p(x) $}
			\begin{theorem}
				Voor het verschil tussen $ f(x) $ en $ p(x) $ geldt het volgende:
				\[ 
					f(x) - p(x) = \half (x-x_0)(x-x_1)f''(\xi) \qquad \xi \in (x_0,x,x_1)\,.
				\]
				Oftewel, $ \xi $ ligt in het open interval opgespannen door de uitersten van $ x_0, x, x_1 $ (we sluiten extrapolatie niet uit).
			\end{theorem}
			
			Voor een verstoorde $f$ schrijven we $\hat{f}$. Definieer nu $ \epsilon_0 > 0 $ en $ \epsilon_1 > 0 $ de storing op $ f(x_0) $ en $ f(x_1) $ door $ \abs{\hat{f}(x_0) - f(x_0)}\leq \epsilon_0 $ en $ \abs{\hat{f}(x_1) - f(x_1)} \leq \epsilon_1 $, waarbij $ f(x_0) $ en $ f(x_1) $ de exacte waardes zijn. Definieer vervolgens $ \epsilon = \max (\epsilon_0, \epsilon_1) $. Dan volgt voor de verstoring op $ p(x) $
			\[ 
				\abs{\hat{p}(x) - p(x)} \leq \abs{\frac{x-x_1}{x_0-x_1}}\epsilon_0 + \abs{\frac{x-x_0}{x_1-x_0}}\epsilon_1 = \frac{\abs{x-x_1}\epsilon_0+\abs{x-x_0}\epsilon_1}{x_1-x_0} \leq \frac{\abs{x-x_1}+\abs{x-x_0}}{x_1-x_0}\epsilon\,.
			\]
			Voor interpolatie is $ \abs{\hat{p}(x) - p(x)} \leq \epsilon$. Voor extrapolatie is dit $ \abs{\hat{p}(x) - p(x)} \leq (1 + 2 \frac{\abs{x-x_1}}{x_1-x_0})\epsilon $. We zien dus dat interpolatie veilig is en de fout beperkt blijft. 
		
		\subsection{Lagrange interpolatie}
			\begin{define}
				Bij \indx{Lagrange interpolatie} is het interval waarover ge\"interpoleerd wordt verdeeld in $n$ deelintervallen. Het interpolatiepolynoom $ L_n(x) $ is gegeven door
				\[ 
					L_n(x) = \sum_{k=0}^n L_{kn} (x) f(x_k), \qquad \text{ met $ L_{kn}$ de \indx{Lagrange co\"efficienten}. }
				\]
				Deze Lagrange co\"efficienten worden gegeven door
				\[ 
					L_{kn}(x) = \frac{(x-x_0)(x-x_1)\dotsm(x-x_{k-1})(x-x_{k+1})\dotsm(x-x_n)}{(x_k-x_0)(x_k-x_1)\dotsm(x_k-x_{k-1})(x_k-x_{k+1})\dotsm(x_k-x_n)}\,.
				\]
				Er volgt 
				$\begin{cases}
					L_{kn}(x_j) = 1 & k = j \\
					L_{kn}(x_j) \neq 1 & k \neq j
				\end{cases} \ $.
				
				Voor de fout (die wordt veroorzaakt door verstoringen in functiewaarden) geldt nu
				\[ 
					\abs{\hat{L}_n(x)-L_n(x)} \leq \sum_{k=0}^n \abs{L_{kn}(x)} \epsilon, \qquad \text{met } \epsilon = \max (\epsilon_0, \epsilon_1, \dotsc, \epsilon_n).
				\]
			\end{define}
			\begin{note}
				Lineaire interpolatie is Lagrange interpolatie voor $n=1$, oftewel \indx{eerste orde Lagrange interpolatie}.
			\end{note}
	
		%TODO stuk tussen Lagrange and Cubic splines...
		\subsection{Kubische splines (\indx{Cubic splines})}
			
			\begin{define}
				\indx{Stuksgewijze interpolatie} houdt in dat het interval waarvoor we een interpolatiepolynoom willen beschrijven wordt opgedeeld in kleinere deelintervallen en we vervolgens voor elk deelinterval een interpolatiepolynoom beschrijven. Het gevolg hiervan is dat we knikken krijgen.
			\end{define}
			
			\begin{define}
				%TODO plaatje
				We willen een interpolatiepolynoom beschrijven voor de functie $f(x)$ op het interval $(a,b)$. We delen het interval $(a,b)$ op in $n$ deelintervallen, $(x_0,x_1), \dotsc, (x_{n-1}, x_n)$. Op elk deelinterval beschrijven we een \indx{kubische spline}, $S_0, S_1, \dotsc, S_{n-1}$. 
				\[ 
					S_j(x) = a_j(x-x_j)^3 + b_j(x-x_j)^2 + c_j(x-x_j) + d_j, \qquad \text{met } j = 0, \dotsc, n-1\,.
				\]
				We hebben nu $n-1$ vergelijkingen, en dus $4n$ onbekenden. Om deze onbekenden te vinden, stellen we een aantal eisen/voorwaarden aan $ S_j(x) $.
				\begin{align*}
					\textbf{Functiewaarden: } &S_j(x_j) = f(x_j) \\
						&S_{n-1}(x_n) = f(x_n) \\
					\textbf{Aansluitvoorwaarden: } &S_j(x_{j+1}) = S_{j+1} (x_{j+1}) \\
						&S_j '(x_{j+1}) = S_{j+1} ' (x_{j+1}) \\
						&S_j '' (x_{j+1}) = S_{j+1} '' (x_{j+1}) \\
					\textbf{\"Randvoorwaarden\": } &S_0 ''(x_0) = 0 \\
						&S_{n-1} '' (x_n) = 0
				\end{align*}
			\end{define}
			
		\newpage
		\section{College 3}
		\subsection{\indx{Eindige differenties} (benaderen van afgeleiden)}
		
			\begin{tikzpicture}
			\begin{axis}[
			axis lines = left,
			xmin=-2, xmax=2, ymin=-3.5, ymax=0.5,
			ylabel=$f(x)$,
			ytick={2,4},
			yticklabels={$f(x_0)$,$f(x_1)$},
			xlabel={$x$},
			xtick={0,0.5},
			xticklabels={$x$, $x+h$}
			]
				\addplot[samples=200]{-x^2};
				\addplot[dashed] coordinates {(0,-3.5) (0,0)};
				\addplot[dashed] coordinates {(0.5,-3.5) (0.5,-0.25)};
				\addplot[] coordinates {(-0.75,0) (0.75,0)} node[above]{$f'(x)$};
			\end{axis}
			\end{tikzpicture}
			
			\begin{define}
				De afgeleide van $ f(x) $ is gedefinieerd als $ f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{(x+h) - x} = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} $. Voor een voorwaartse benadering van de afgeleide definieren we \index{Qv@$Q_v(x,h)$}
				\[ 
					Q_v(x,h) = \frac{f(x+h)-f(x)}{h}, \qquad \text{met $h$ eindig} \,.
				\]
				Het verschil tussen deze benadering en de werkelijke afgeleide noemen we de \indx{afbreekfout}. Deze is gegeven door
				\[ 
					f'(x) - Q_v(x,h) = f'(x) -  \frac{f(x+h)-f(x)}{h} = - \frac{h}{2} f ''(\xi), \qquad \text{voor } f \in C[x,x+h], \xi \in (x,h)\,.
				\]
				
				Verder bekijken we de achterwaartse benadering \index{Qa@$Q_a(x,h)$}
				\[ 
					 Q_a(x,h) = \frac{f(x)-f(x-h)}{h}, \qquad \text{met $h$ eindig} ,
				\]
				met bijbehorende afbreekfout $ f'(x) - Q_a(x,h) = \frac{h}{2} f ''(\eta) $, $\eta \in (x-h,h)$.
				
				Tenslotte bekijken we de centrale benadering, wat het gemiddelde is van de voor- en achterwaartse benaderingen. \index{Qc@$Q_c(x,h)$}
				\[ 
					Q_c(x,h) = \half (Q_v(x,h) + Q_a(x,h)) = \frac{f(x+h)-f(x-h)}{2h}\,.
				\]
				De afbreekfout die volgt is $ f'(x) - Q_c(x,h) = \frac{-h^2}{6} f'''(\xi), \xi \in (x-h, x+h) $.
			\end{define}
	
	
	\newpage
	\section{College 4}
		We bekijken differentiaalvergelijkingen van de vorm 
		\[ 
			y' = \frac{\d y}{\d t} = f(t,y) \,, y = y(t) \,, \text{ beginvoorwaarde } y(t_0) = y_0 \,,
		 \]
		 met $f(t,y),t_0,y_0$ gegeven.
		 
		 \begin{voorbeeld}
		 	
		 	$y'=\lambda y,\ y(t_0)=y(0)=y_0$ is nog exact op te lossen, en heeft als oplossing $y=y_0 e^{\lambda t}$. We kiezen hier voor het gemak $\lambda <0$, omdat we daardoor fysische stabiliteit garanderen.
		 \end{voorbeeld}
		 \begin{define}
		 	$y'=f(t,y)$ is \indx{goed gesteld} \index{well posed}indien er een unieke oplossing $y=y(t)$ bestaat die continu afhangt van $y(t_0)=y_0$.
		 	
		 	Een karakterisering is 
		 	\[ 
			 	\abs{\frac{\partial f(t,y)}{\partial y}} \le L \in \reals \ \forall{t,y} \text{ in het te beschouwen rekendomein (t-waarden)}.
		 	 \]
		 	 Dit kennen we ook als \indx{Lipschitz-continu\"iteit}.
		 \end{define}
		 
		 \begin{define}
		 	We noemen $y'=f(t,y)$ de \indx{differentiaalvorm}, $y(t_0) = y_0$. Dit is om te vormen in $y(t) = y_0 + \int_{t_0}^t f(\tau,y) \d \tau$, de \indx{integraalvorm} (impliciet).
		 \end{define}			
		 
		 \subsection{Numerieke methoden voor differentiaalvergelijkingen}
		 \subsubsection{Voorwaarts Euler (instabiel)}
			 We benaderen $y(t) \approx y_0 + h f(t_0,y_0)$ met $h=t-t_0$, stukgewijs. Dus $y(t_1) \approx y_0 + h f(t_0,y_0) = w_1$, $y(t_1)$ is de exacte oplossing en $w_1$ de numerieke, en $w_2 = w_1 + h f(t_1,w_1)$, enzovoort.
			 
			 Algemener geldt dan $w_{j+1}+h f(t_j,w_j)$ met $h=t_{j+1}-t_j$, dit is dus een expliciete formule.
		\subsubsection{Achterwaarts Euler (iets stabieler)}
			Hier geldt de impliciete formule 
			\[ 
				w_{j+1} = w_j + h f(t_{j+1},w_{j+1}) \,.
			 \]
		\subsubsection{Trapeziumregel}
			Weer een impliciete formule
			\[ 
				w_{j+1} = w_j + \frac{h}{2} ( f(t_j,w_j) + f(t_{j+1},w_{j+1})) \,.
			 \]
		\subsubsection{Modified Euler}
			Dit is een expliciete vorm van de trapeziumregel. Het doet eerst een voorspelling met voorwaarts Euler, $\overline w_{j+1} = w_j + h f(t_j,w_j)$. Dan volgt een correctiestap $w_{j+1} = w_j + \frac{h}{2} (f(t_j,w_j) + f(t_{j+1},\overline w_{j+1}))$ .
			
		\subsection{Versterkingsfactoren van de methoden}
			toegepast op $y'=\lambda y,\ y(t_0) = y_0$.
			\paragraph{Voorwaarts Euler} 
				\begin{define}
				Er geldt dus $w_{j+1} = w_j + h \lambda w_j = (1+\lambda h) w_j$, waarbij de \indx{versterkingsfactor} $Q(\lambda,h) = 1+\lambda h$.
				\end{define}
			Je kunt nagaan dat de andere versterkingsfactoren respectievelijk 
			\[ 
				\frac{1}{1-\lambda h},\ \frac{1+\frac{h}{2}\lambda}{1-\frac{h}{2}\lambda},\ 1+h\lambda + \frac{1}{2} (h\lambda)^2
				\]
				zijn.
				
		\subsection{Verstoringen in beginwaarden}
			\begin{define}
			Gegeven de vergelijkingen
			\begin{align}
				y'=\lambda y,\ & y(t_0) = y_0 \\
				\hat y' = \lambda \hat y,\ & \hat y (t_0) = \hat y_0
			\end{align}
			met $\epsilon_0 = \hat y_0 - y_0$ als verstoring in beginvoorwaarde, dan volgt uit $(2)-(1)$
			\[ 
				\hat y' - y'=\lambda (\hat y-y) \implies \epsilon'=\lambda \epsilon,\ \epsilon(t_0) = \epsilon_0 
			 \]
			 waarbij $\epsilon'=\lambda \epsilon$ de \indx{evolutievergelijking} voor de verstoring is.
			 \end{define}
			 
			 %todo?
			 
		\subsection{\indx{Local truncation error} }
			\begin{define}
			De \indx{lokale afbreekfout} $\tau_{j+1}=\frac{y_{j+1}-z_{j+1}}{h}$ met $z_{j+1}$ de numerieke oplossing uitgaande van de exacte oplossing.
			\end{define}
			
			%todo
			
			\begin{define}
				
				$e_{j+1}=w_{j+1}-y_{j+1}$ is de \indx{lokale discretisatiefout} (de fout in de oplossing).
			\end{define}
			$\tau_{j+1}$ is een maat voor de fout in de evaluatie.
		
			
				
						 
	\section{College 5}
	
	\section{College 6}
	
	\section{College 7}
	
	\section{College 8}
			 
			 
	\phantomsection
	\newpage
	\addcontentsline{toc}{chapter}{Index} 
	\printindex
	
	
\end{document} 
